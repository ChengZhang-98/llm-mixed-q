[default]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 6
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer.self_attn.q_proj]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]

[model_layer.self_attn.k_proj]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]

[model_layer.self_attn.v_proj]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 7
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]

[model_layer.self_attn.o_proj]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]

[model_layer.self_attn.rotary_positional_encoding]
bypass = false
name = "integer"
data_in_width = 4
data_in_frac_width = 3

[model_layer.self_attn.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 7
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 6
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]

[model_layer.self_attn.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 6
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]

[model_layer.mlp.gate_proj]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]

[model_layer.mlp.down_proj]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]

[model_layer.mlp.up_proj]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 7
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
