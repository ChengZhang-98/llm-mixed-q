by = "name"

[default]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]
