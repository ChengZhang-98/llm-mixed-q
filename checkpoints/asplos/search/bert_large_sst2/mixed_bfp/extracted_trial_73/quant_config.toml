[default]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_12.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_12.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_12.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_12.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_12.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_12.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_12.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_13.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_13.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_13.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_13.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_13.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_13.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_13.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_14.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_14.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_14.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_14.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_14.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_14.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_14.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_15.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_15.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_15.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_15.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_15.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_15.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_15.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_16.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_16.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_16.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_16.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_16.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_16.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_16.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_17.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_17.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_17.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_17.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_17.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_17.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_17.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_18.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_18.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_18.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_18.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_18.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_18.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_18.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_19.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_19.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_19.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_19.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_19.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_19.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_19.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_20.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_20.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_20.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_20.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_20.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_20.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_20.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_21.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_21.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_21.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_21.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_21.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_21.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_21.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_22.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_22.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_22.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_22.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_22.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_22.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_22.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_23.attention.query]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_23.attention.key]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_23.attention.value]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_23.attention.matmul_0]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_23.attention.matmul_1]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_23.intermediate.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_23.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_0.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_1.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_2.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_3.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_4.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_5.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_6.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_7.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_8.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_9.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_10.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_11.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_12.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_13.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_14.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_15.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_16.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_17.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 3
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_18.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 3
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 5
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_19.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_20.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 5
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 2
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_21.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 4
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_22.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 6
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 2
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 4
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]

[model_layer_23.attention.output.dense]
name = "block_fp"
bypass = false
is_ptq = true
data_in_width = 5
data_in_exponent_width = 8
data_in_exponent_bias = "NA"
data_in_block_size = [ 1, 16,]
weight_width = 4
weight_exponent_width = 8
weight_exponent_bias = "NA"
weight_block_size = [ 1, 16,]
bias_width = 3
bias_exponent_width = 8
bias_exponent_bias = "NA"
bias_block_size = [ 1, 16,]
