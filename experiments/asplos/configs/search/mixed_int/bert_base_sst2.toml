[search_strategy]
n_jobs = 1
n_trials = 128
timeout = 28800.0
sampler = "TPE"
accuracy_threshold = 0.6
avg_bitwidth_threshold = 8.0
fps_threshold = 0.0
fps_per_lut_threshold = 0.0
sort_by = ["accuracy", "avg_bitwidth", "fps", "fps_per_lut"]

[search_estimator]
alpha_accuracy = 1
alpha_memory_density = 0.1
alpha_fps = 0.0
alpha_fps_per_lut = 0.0
compare_to = 32

[search_space]
extend_quant_config_seed_first = true
[search_space.quant_config_seed]
[search_space.quant_config_seed.default]
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = [16, 8]
data_in_frac_width = ["!ast!None"]
weight_width = [8, 4]
weight_frac_width = ["!ast!None"]
bias_width = [8, 4]
bias_frac_width = ["!ast!None"]

[search_space.quant_config_seed.model_layer.attention.query]
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = [16, 8]
data_in_frac_width = ["!ast!None"]
weight_width = [8, 4]
weight_frac_width = ["!ast!None"]
bias_width = [8, 4]
bias_frac_width = ["!ast!None"]
data_out_width = [16, 8]
data_out_frac_width = ["!ast!None"]

[search_space.quant_config_seed.model_layer.attention.key]
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = [16, 8]
data_in_frac_width = ["!ast!None"]
weight_width = [8, 4]
weight_frac_width = ["!ast!None"]
bias_width = [8, 4]
bias_frac_width = ["!ast!None"]
data_out_width = [16, 8]
data_out_frac_width = ["!ast!None"]

[search_space.quant_config_seed.model_layer.attention.value]
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = [16, 8]
data_in_frac_width = ["!ast!None"]
weight_width = [8, 4]
weight_frac_width = ["!ast!None"]
bias_width = [8, 4]
bias_frac_width = ["!ast!None"]
data_out_width = [16, 8]
data_out_frac_width = ["!ast!None"]

[search_space.quant_config_seed.model_layer.attention.matmul_0]
# Q @ K^T, where Q is the layer query's output and K is the layer key's output
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = ["!ast!None"]
data_in_frac_width = ["!ast!None"]
weight_width = ["!ast!None"]
weight_frac_width = ["!ast!None"]

[search_space.quant_config_seed.model_layer.attention.matmul_1]
# A_hat @ V, where A_hat is the softmax output and V is the layer value's output
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = [16, 8] # softmax output
data_in_frac_width = ["!ast!None"]
weight_width = ["!ast!None"]
weight_frac_width = ["!ast!None"]

[search_space.quant_config_seed.model_layer.attention.output.dense]
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = [16, 8]
data_in_frac_width = ["!ast!None"]
weight_width = [8, 4]
weight_frac_width = ["!ast!None"]
bias_width = [8, 4]
bias_frac_width = ["!ast!None"]

[search_space.quant_config_seed.model_layer.intermediate.dense]
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = [16, 8]
data_in_frac_width = ["!ast!None"]
weight_width = [8, 4]
weight_frac_width = ["!ast!None"]
bias_width = [8, 4]
bias_frac_width = ["!ast!None"]

[search_space.quant_config_seed.model_layer.output.dense]
name = ["integer"]
bypass = ["!ast!False"]
is_ptq = ["!ast!True"]
data_in_width = [16, 8]
data_in_frac_width = ["!ast!None"]
weight_width = [8, 4]
weight_frac_width = ["!ast!None"]
bias_width = [8, 4]
bias_frac_width = ["!ast!None"]
